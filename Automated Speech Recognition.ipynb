{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dataloader\n!pip install fairseq\n!pip install pip==24.0\n!pip install hydra-core==1.0.7 omegaconf==2.0.6\n!git clone https://github.com/pytorch/fairseq\n!pip install --editable ./fairseq -v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:33:03.789559Z","iopub.execute_input":"2024-12-26T20:33:03.789880Z","iopub.status.idle":"2024-12-26T20:42:10.819294Z","shell.execute_reply.started":"2024-12-26T20:33:03.789849Z","shell.execute_reply":"2024-12-26T20:42:10.818097Z"}},"outputs":[{"name":"stdout","text":"Collecting dataloader\n  Downloading dataloader-2.0.tar.gz (9.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: dataloader\n  Building wheel for dataloader (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for dataloader: filename=dataloader-2.0-py3-none-any.whl size=10085 sha256=028cd8eb6feefca46c69a6e7d2b2d47bdbbe3adb04489d074db5da90c030f658\n  Stored in directory: /root/.cache/pip/wheels/60/56/53/2b1c14a2abb6f40f1d59f97461a59e61f326433fac416794de\nSuccessfully built dataloader\nInstalling collected packages: dataloader\nSuccessfully installed dataloader-2.0\nCollecting fairseq\n  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.17.1)\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.11)\nCollecting hydra-core<1.1,>=1.0.7 (from fairseq)\n  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting omegaconf<2.1 (from fairseq)\n  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0m  Downloading omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0m  Downloading omegaconf-2.0.4-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.4 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/92/b1/4f3023143436f12c98bab53f0b3db617bd18a7d223627d5030e13a7b4fc2/omegaconf-2.0.4-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0m  Downloading omegaconf-2.0.3-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.3 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/29/08/a88210c2c1aa0a3f65f05d8a6c98939ccb84b6fb982aa6567dec4e6773f9/omegaconf-2.0.3-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0m  Downloading omegaconf-2.0.2-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.2 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/72/fe/f8d162aa059fb4f327fd75144dd69aa7e8acbb6d8d37013e4638c8490e0b/omegaconf-2.0.2-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0m  Downloading omegaconf-2.0.1-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.1 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/86/ec/605805e60abdb025b06664d107335031bb8ebdc52e0a90bdbad6a7130279/omegaconf-2.0.1-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0m  Downloading omegaconf-2.0.0-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.26.4)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.9.11)\nCollecting sacrebleu>=1.4.12 (from fairseq)\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.5)\nCollecting bitarray (from fairseq)\n  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\nRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\nCollecting omegaconf<2.1 (from fairseq)\n  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0m  Using cached omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\nRequested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    PyYAML (>=5.1.*)\n            ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n\u001b[0mINFO: pip is looking at multiple versions of hydra-core to determine which version is compatible with other requirements. This could take a while.\nCollecting fairseq\n  Downloading fairseq-0.12.1.tar.gz (9.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n\n\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\nCollecting pip==24.0\n  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-24.0-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-24.0\nCollecting hydra-core==1.0.7\n  Using cached hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting omegaconf==2.0.6\n  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\nCollecting antlr4-python3-runtime==4.8 (from hydra-core==1.0.7)\n  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (6.0.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (4.12.2)\nDownloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\nBuilding wheels for collected packages: antlr4-python3-runtime\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141213 sha256=852ddd203665bf1786c8c310df8ac22f767ed4933f5cfe3f827e497366202ae9\n  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\nSuccessfully built antlr4-python3-runtime\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n  Attempting uninstall: antlr4-python3-runtime\n    Found existing installation: antlr4-python3-runtime 4.9.3\n    Uninstalling antlr4-python3-runtime-4.9.3:\n      Successfully uninstalled antlr4-python3-runtime-4.9.3\n  Attempting uninstall: omegaconf\n    Found existing installation: omegaconf 2.3.0\n    Uninstalling omegaconf-2.3.0:\n      Successfully uninstalled omegaconf-2.3.0\nSuccessfully installed antlr4-python3-runtime-4.8 hydra-core-1.0.7 omegaconf-2.0.6\nCloning into 'fairseq'...\nremote: Enumerating objects: 35385, done.\u001b[K\nremote: Counting objects: 100% (15/15), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 35385 (delta 6), reused 5 (delta 5), pack-reused 35370 (from 2)\u001b[K\nReceiving objects: 100% (35385/35385), 25.47 MiB | 33.02 MiB/s, done.\nResolving deltas: 100% (25539/25539), done.\nUsing pip 24.0 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\nObtaining file:///kaggle/working/fairseq\n  Running command pip subprocess to install build dependencies\n  Using pip 24.0 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n  Collecting setuptools>=18.0\n    Obtaining dependency information for setuptools>=18.0 from https://files.pythonhosted.org/packages/55/21/47d163f615df1d30c094f6c8bbb353619274edccf0327b185cc2493c2c33/setuptools-75.6.0-py3-none-any.whl.metadata\n    Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n  Collecting wheel\n    Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl.metadata\n    Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n  Collecting cython\n    Obtaining dependency information for cython from https://files.pythonhosted.org/packages/f0/89/b1ae45689abecca777f95462781a76e67ff46b55495a481ec5a73a739994/Cython-3.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n    Using cached Cython-3.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n  Collecting numpy>=1.21.3\n    Obtaining dependency information for numpy>=1.21.3 from https://files.pythonhosted.org/packages/f7/b6/d8110985501ca8912dfc1c3bbef99d66e62d487f72e46b2337494df77364/numpy-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n    Using cached numpy-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n  Collecting torch>=1.10\n    Obtaining dependency information for torch>=1.10 from https://files.pythonhosted.org/packages/2a/ef/834af4a885b31a0b32fff2d80e1e40f771e1566ea8ded55347502440786a/torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata\n    Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n  Collecting filelock (from torch>=1.10)\n    Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl.metadata\n    Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n  Collecting typing-extensions>=4.8.0 (from torch>=1.10)\n    Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n    Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n  Collecting networkx (from torch>=1.10)\n    Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/b9/54/dd730b32ea14ea797530a4479b2ed46a6fb250f682a9cfb997e968bf0261/networkx-3.4.2-py3-none-any.whl.metadata\n    Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n  Collecting jinja2 (from torch>=1.10)\n    Obtaining dependency information for jinja2 from https://files.pythonhosted.org/packages/bd/0f/2ba5fbcd631e3e88689309dbe978c5769e883e4b84ebfe7da30b43275c5a/jinja2-3.1.5-py3-none-any.whl.metadata\n    Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n  Collecting fsspec (from torch>=1.10)\n    Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/de/86/5486b0188d08aa643e127774a99bac51ffa6cf343e3deb0583956dca5b22/fsspec-2024.12.0-py3-none-any.whl.metadata\n    Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n  Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.4.127 from https://files.pythonhosted.org/packages/2c/14/91ae57cd4db3f9ef7aa99f4019cfa8d54cb4caa7e00975df6467e9725a9f/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n  Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cuda-runtime-cu12==12.4.127 from https://files.pythonhosted.org/packages/ea/27/1795d86fe88ef397885f2e580ac37628ed058a92ed2c39dc8eac3adf0619/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n  Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cuda-cupti-cu12==12.4.127 from https://files.pythonhosted.org/packages/67/42/f4f60238e8194a3106d06a058d494b18e006c10bb2b915655bd9f6ea4cb1/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n  Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cudnn-cu12==9.1.0.70 from https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n  Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cublas-cu12==12.4.5.8 from https://files.pythonhosted.org/packages/ae/71/1c91302526c45ab494c23f61c7a84aa568b8c1f9d196efa5993957faf906/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n  Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cufft-cu12==11.2.1.3 from https://files.pythonhosted.org/packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n  Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10)\n    Obtaining dependency information for nvidia-curand-cu12==10.3.5.147 from https://files.pythonhosted.org/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n  Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cusolver-cu12==11.6.1.9 from https://files.pythonhosted.org/packages/3a/e1/5b9089a4b2a4790dfdea8b3a006052cfecff58139d5a4e34cb1a51df8d6f/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n  Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10)\n    Obtaining dependency information for nvidia-cusparse-cu12==12.3.1.170 from https://files.pythonhosted.org/packages/db/f7/97a9ea26ed4bbbfc2d470994b8b4f338ef663be97b8f677519ac195e113d/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n  Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.10)\n    Obtaining dependency information for nvidia-nccl-cu12==2.21.5 from https://files.pythonhosted.org/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n  Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.10)\n    Obtaining dependency information for nvidia-nvtx-cu12==12.4.127 from https://files.pythonhosted.org/packages/87/20/199b8713428322a2f22b722c62b8cc278cc53dffa9705d744484b5035ee9/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n  Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10)\n    Obtaining dependency information for nvidia-nvjitlink-cu12==12.4.127 from https://files.pythonhosted.org/packages/ff/ff/847841bacfbefc97a00036e0fce5a0f086b640756dc38caea5e1bb002655/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n    Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n  Collecting triton==3.1.0 (from torch>=1.10)\n    Obtaining dependency information for triton==3.1.0 from https://files.pythonhosted.org/packages/98/29/69aa56dc0b2eb2602b553881e34243475ea2afd9699be042316842788ff5/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n    Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n  Collecting sympy==1.13.1 (from torch>=1.10)\n    Obtaining dependency information for sympy==1.13.1 from https://files.pythonhosted.org/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl.metadata\n    Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n  Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.10)\n    Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n    Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n  Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.10)\n    Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/22/35/137da042dfb4720b638d2937c38a9c2df83fe32d20e8c8f3185dbfef05f7/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n    Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n  Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n  Using cached Cython-3.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n  Using cached numpy-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 906.4/906.4 MB 1.8 MB/s eta 0:00:00\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 1.7 MB/s eta 0:00:00\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 94.6 MB/s eta 0:00:00\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 22.4 MB/s eta 0:00:00\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 27.5 MB/s eta 0:00:00\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.0 MB/s eta 0:00:00\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 8.1 MB/s eta 0:00:00\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 30.0 MB/s eta 0:00:00\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.7 MB/s eta 0:00:00\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 8.2 MB/s eta 0:00:00\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 MB 8.8 MB/s eta 0:00:00\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 17.0 MB/s eta 0:00:00\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 6.7 MB/s eta 0:00:00\n  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 101.6 MB/s eta 0:00:00\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.5/209.5 MB 7.6 MB/s eta 0:00:00\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.9/183.9 kB 10.5 MB/s eta 0:00:00\n  Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.6/134.6 kB 9.9 MB/s eta 0:00:00\n  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 64.4 MB/s eta 0:00:00\n  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 29.8 MB/s eta 0:00:00\n  DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n  Installing collected packages: mpmath, wheel, typing-extensions, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, cython, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n    Creating /tmp/pip-build-env-c0rz84oa/overlay/local/bin\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/wheel to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/isympy to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/f2py to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/numpy-config to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/cygdb to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/cython to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/cythonize to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/proton to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/proton-viewer to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/convert-caffe2-to-onnx to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/convert-onnx-to-caffe2 to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/torchfrtrace to 755\n    changing mode of /tmp/pip-build-env-c0rz84oa/overlay/local/bin/torchrun to 755\n  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n  catboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.2.1 which is incompatible.\n  cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.2.1 which is incompatible.\n  cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\n  cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.2.1 which is incompatible.\n  datasets 3.2.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n  distributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\n  fastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1 which is incompatible.\n  gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.12.0 which is incompatible.\n  gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.1 which is incompatible.\n  ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.2.1 which is incompatible.\n  ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\n  numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.1 which is incompatible.\n  pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.2.1 which is incompatible.\n  pandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n  pytensor 2.25.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.1 which is incompatible.\n  rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.2.1 which is incompatible.\n  tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.1 which is incompatible.\n  thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.1 which is incompatible.\n  torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\n  torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\n  ydata-profiling 4.12.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.1 which is incompatible.\n  Successfully installed MarkupSafe-3.0.2 cython-3.0.11 filelock-3.16.1 fsspec-2024.12.0 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 setuptools-75.6.0 sympy-1.13.1 torch-2.5.1 triton-3.1.0 typing-extensions-4.12.2 wheel-0.45.1\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Running command Checking if build backend supports build_editable\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Running command Getting requirements to build editable\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'test_suite'\n    warnings.warn(msg)\n  running egg_info\n  creating fairseq.egg-info\n  writing fairseq.egg-info/PKG-INFO\n  writing dependency_links to fairseq.egg-info/dependency_links.txt\n  writing entry points to fairseq.egg-info/entry_points.txt\n  writing requirements to fairseq.egg-info/requires.txt\n  writing top-level names to fairseq.egg-info/top_level.txt\n  writing manifest file 'fairseq.egg-info/SOURCES.txt'\n  reading manifest file 'fairseq.egg-info/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  adding license file 'LICENSE'\n  writing manifest file 'fairseq.egg-info/SOURCES.txt'\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Running command Preparing editable metadata (pyproject.toml)\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'test_suite'\n    warnings.warn(msg)\n  running dist_info\n  creating /tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info\n  writing /tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/PKG-INFO\n  writing dependency_links to /tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/dependency_links.txt\n  writing entry points to /tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/entry_points.txt\n  writing requirements to /tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/requires.txt\n  writing top-level names to /tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/top_level.txt\n  writing manifest file '/tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/SOURCES.txt'\n  reading manifest file '/tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  adding license file 'LICENSE'\n  writing manifest file '/tmp/pip-modern-metadata-ply2_tih/fairseq.egg-info/SOURCES.txt'\n  creating '/tmp/pip-modern-metadata-ply2_tih/fairseq-0.12.2.dist-info'\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.17.1)\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.11)\nRequirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.0.7)\nRequirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.6)\nRequirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.26.4)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2024.9.11)\nCollecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n  Obtaining dependency information for sacrebleu>=1.4.12 from https://files.pythonhosted.org/packages/15/d8/e51d35bc863caa19ddeae48dfb890581a19326973ad1c9fa5dcfc63310f7/sacrebleu-2.4.3-py3-none-any.whl.metadata\n  Using cached sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\nRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.5)\nCollecting bitarray (from fairseq==0.12.2)\n  Obtaining dependency information for bitarray from https://files.pythonhosted.org/packages/17/33/c2a7cb6f0030ea94408c84c4f80f4065b54b2bf1d4080e36fcd0b4c587a2/bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\nRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.1)\nRequirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (4.8)\nRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.12.2)\nCollecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/3d/4c/4cb6bb4061910ac74c444be76e7d17dba97d9057030cca2f96947c3f7a0f/portalocker-3.0.0-py3-none-any.whl.metadata\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2024.6.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: fairseq\n  Running command Building editable for fairseq (pyproject.toml)\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'test_suite'\n    warnings.warn(msg)\n  running editable_wheel\n  creating /tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info\n  writing /tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/PKG-INFO\n  writing dependency_links to /tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/dependency_links.txt\n  writing entry points to /tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/entry_points.txt\n  writing requirements to /tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/requires.txt\n  writing top-level names to /tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/top_level.txt\n  writing manifest file '/tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/SOURCES.txt'\n  reading manifest file '/tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  adding license file 'LICENSE'\n  writing manifest file '/tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq.egg-info/SOURCES.txt'\n  creating '/tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq-0.12.2.dist-info'\n  creating /tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq-0.12.2.dist-info/WHEEL\n  running build_py\n  running build_ext\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n  building 'fairseq.libbleu' extension\n  creating /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbleu\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbleu/module.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/libbleu/module.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n  [2/2] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbleu/libbleu.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/libbleu/libbleu.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n  creating /tmp/tmp3yke0n0e.build-lib/fairseq\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbleu/libbleu.o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbleu/module.o -L/usr/lib/x86_64-linux-gnu -o /tmp/tmp3yke0n0e.build-lib/fairseq/libbleu.cpython-310-x86_64-linux-gnu.so\n  [1/1] Cythonizing fairseq/data/data_utils_fast.pyx\n  building 'fairseq.data.data_utils_fast' extension\n  creating /tmp/tmpeofm26wo.build-temp/fairseq/data\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/1] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/data/data_utils_fast.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/data/data_utils_fast.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/ndarraytypes.h:1913,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/ndarrayobject.h:12,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/arrayobject.h:5,\n                   from /kaggle/working/fairseq/fairseq/data/data_utils_fast.cpp:1273:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n     17 | #warning \"Using deprecated NumPy API, disable it with \" \\\n        |  ^~~~~~~\n  creating /tmp/tmp3yke0n0e.build-lib/fairseq/data\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/fairseq/data/data_utils_fast.o -L/usr/lib/x86_64-linux-gnu -o /tmp/tmp3yke0n0e.build-lib/fairseq/data/data_utils_fast.cpython-310-x86_64-linux-gnu.so\n  [1/1] Cythonizing fairseq/data/token_block_utils_fast.pyx\n  building 'fairseq.data.token_block_utils_fast' extension\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/1] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/data/token_block_utils_fast.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/data/token_block_utils_fast.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/ndarraytypes.h:1913,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/ndarrayobject.h:12,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/arrayobject.h:5,\n                   from /kaggle/working/fairseq/fairseq/data/token_block_utils_fast.cpp:1274:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/numpy/_core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n     17 | #warning \"Using deprecated NumPy API, disable it with \" \\\n        |  ^~~~~~~\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/fairseq/data/token_block_utils_fast.o -L/usr/lib/x86_64-linux-gnu -o /tmp/tmp3yke0n0e.build-lib/fairseq/data/token_block_utils_fast.cpython-310-x86_64-linux-gnu.so\n  building 'fairseq.libbase' extension\n  creating /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbase\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/1] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbase/balanced_assignment.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/libbase/balanced_assignment.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbase/balanced_assignment.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbase -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/fairseq/clib/libbase/balanced_assignment.o -L/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o /tmp/tmp3yke0n0e.build-lib/fairseq/libbase.cpython-310-x86_64-linux-gnu.so\n  building 'fairseq.libnat' extension\n  creating /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/1] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat/edit_dist.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/libnat/edit_dist.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat/edit_dist.o -L/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o /tmp/tmp3yke0n0e.build-lib/fairseq/libnat.cpython-310-x86_64-linux-gnu.so\n  building 'alignment_train_cpu_binding' extension\n  creating /tmp/tmpeofm26wo.build-temp/examples/operators\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/1] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_cpu.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp -o /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=alignment_train_cpu_binding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = double]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:131:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    131 |   free(cumprod_1mp);\n        |   ~~~~^~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:93:20: note: returned from ‘void* operator new [](std::size_t)’\n     93 |   T* cumprod_1mp = new T[elements];\n        |                    ^~~~~~~~~~~~~~~\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = double]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:132:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    132 |   free(cumprod_1mp_clamp);\n        |   ~~~~^~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:94:26: note: returned from ‘void* operator new [](std::size_t)’\n     94 |   T* cumprod_1mp_clamp = new T[elements];\n        |                          ^~~~~~~~~~~~~~~\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = float]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:131:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    131 |   free(cumprod_1mp);\n        |   ~~~~^~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:93:20: note: returned from ‘void* operator new [](std::size_t)’\n     93 |   T* cumprod_1mp = new T[elements];\n        |                    ^~~~~~~~~~~~~~~\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = float]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:132:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    132 |   free(cumprod_1mp_clamp);\n        |   ~~~~^~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:94:26: note: returned from ‘void* operator new [](std::size_t)’\n     94 |   T* cumprod_1mp_clamp = new T[elements];\n        |                          ^~~~~~~~~~~~~~~\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::Half]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:131:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    131 |   free(cumprod_1mp);\n        |   ~~~~^~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:93:20: note: returned from ‘void* operator new [](std::size_t)’\n     93 |   T* cumprod_1mp = new T[elements];\n        |                    ^~~~~~~~~~~~~~~\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::Half]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:132:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    132 |   free(cumprod_1mp_clamp);\n        |   ~~~~^~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:94:26: note: returned from ‘void* operator new [](std::size_t)’\n     94 |   T* cumprod_1mp_clamp = new T[elements];\n        |                          ^~~~~~~~~~~~~~~\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::BFloat16]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:131:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    131 |   free(cumprod_1mp);\n        |   ~~~~^~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:93:20: note: returned from ‘void* operator new [](std::size_t)’\n     93 |   T* cumprod_1mp = new T[elements];\n        |                    ^~~~~~~~~~~~~~~\n  In function ‘void {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::BFloat16]’,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3,\n      inlined from ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’ at /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:143:3:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:132:7: warning: ‘void free(void*)’ called on pointer returned from a mismatched allocation function [-Wmismatched-new-delete]\n    132 |   free(cumprod_1mp_clamp);\n        |   ~~~~^~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp: In function ‘void {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cpu.cpp:94:26: note: returned from ‘void* operator new [](std::size_t)’\n     94 |   T* cumprod_1mp_clamp = new T[elements];\n        |                          ^~~~~~~~~~~~~~~\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_cpu.o -L/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o /tmp/tmp3yke0n0e.build-lib/alignment_train_cpu_binding.cpython-310-x86_64-linux-gnu.so\n  building 'fairseq.libnat_cuda' extension\n  creating /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat_cuda\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n    warnings.warn(\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat_cuda/binding.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat_cuda/binding.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libnat_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp: In function ‘at::Tensor LevenshteinDistance(at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:34:3: note: in expansion of macro ‘CHECK_INPUT’\n     34 |   CHECK_INPUT(source);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:35:3: note: in expansion of macro ‘CHECK_INPUT’\n     35 |   CHECK_INPUT(target);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:36:3: note: in expansion of macro ‘CHECK_INPUT’\n     36 |   CHECK_INPUT(source_length);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:37:3: note: in expansion of macro ‘CHECK_INPUT’\n     37 |   CHECK_INPUT(target_length);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp: In function ‘at::Tensor GenerateDeletionLabel(at::Tensor, at::Tensor)’:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:44:3: note: in expansion of macro ‘CHECK_INPUT’\n     44 |   CHECK_INPUT(source);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:45:3: note: in expansion of macro ‘CHECK_INPUT’\n     45 |   CHECK_INPUT(operations);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp: In function ‘std::pair<at::Tensor, at::Tensor> GenerateInsertionLabel(at::Tensor, at::Tensor)’:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:52:3: note: in expansion of macro ‘CHECK_INPUT’\n     52 |   CHECK_INPUT(target);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/BlasBackend.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Context.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:22:3: note: in expansion of macro ‘TORCH_CHECK’\n     22 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:26:3: note: in expansion of macro ‘CHECK_CUDA’\n     26 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:53:3: note: in expansion of macro ‘CHECK_INPUT’\n     53 |   CHECK_INPUT(operations);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/ivalue.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List_inl.h:4,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/List.h:488,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef_inl.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/IListRef.h:631,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/DeviceGuard.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:9,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /kaggle/working/fairseq/fairseq/clib/libnat_cuda/binding.cpp:14:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat_cuda/edit_dist.o.d -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/libnat_cuda/edit_dist.cu -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat_cuda/edit_dist.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libnat_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat_cuda/binding.o /tmp/tmpeofm26wo.build-temp/fairseq/clib/libnat_cuda/edit_dist.o -L/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o /tmp/tmp3yke0n0e.build-lib/fairseq/libnat_cuda.cpython-310-x86_64-linux-gnu.so\n  building 'fairseq.ngram_repeat_block_cuda' extension\n  creating /tmp/tmpeofm26wo.build-temp/fairseq/clib/cuda\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n    warnings.warn(\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/fairseq/clib/cuda/ngram_repeat_block_cuda.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/cuda/ngram_repeat_block_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=ngram_repeat_block_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/core/Device.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:6:\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp: In function ‘at::Tensor ngram_repeat_block_forward(at::Tensor, at::Tensor, int, int, int, int)’:\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:23:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     23 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:23:3: note: in expansion of macro ‘TORCH_CHECK’\n     23 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:27:3: note: in expansion of macro ‘CHECK_CUDA’\n     27 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:39:3: note: in expansion of macro ‘CHECK_INPUT’\n     39 |   CHECK_INPUT(tokens);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:6:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/core/Device.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:6:\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:23:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     23 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |                     ^\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:23:3: note: in expansion of macro ‘TORCH_CHECK’\n     23 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:27:3: note: in expansion of macro ‘CHECK_CUDA’\n     27 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:40:3: note: in expansion of macro ‘CHECK_INPUT’\n     40 |   CHECK_INPUT(lprobs);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp:6:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmpeofm26wo.build-temp/fairseq/clib/cuda/ngram_repeat_block_cuda_kernel.o.d -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/fairseq/clib/cuda/ngram_repeat_block_cuda_kernel.cu -o /tmp/tmpeofm26wo.build-temp/fairseq/clib/cuda/ngram_repeat_block_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=ngram_repeat_block_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/fairseq/clib/cuda/ngram_repeat_block_cuda.o /tmp/tmpeofm26wo.build-temp/fairseq/clib/cuda/ngram_repeat_block_cuda_kernel.o -L/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o /tmp/tmp3yke0n0e.build-lib/fairseq/ngram_repeat_block_cuda.cpython-310-x86_64-linux-gnu.so\n  building 'alignment_train_cuda_binding' extension\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n    warnings.warn(\n  Emitting ninja build file /tmp/tmpeofm26wo.build-temp/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_cuda.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp -o /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=alignment_train_cuda_binding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/core/Device.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.h:11,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp:9:\n  /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp: In function ‘void {anonymous}::alignmentTrainCUDA(const at::Tensor&, at::Tensor&, float)’:\n  /kaggle/working/fairseq/examples/operators/utils.h:14:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     14 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/utils.h:14:3: note: in expansion of macro ‘TORCH_CHECK’\n     14 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/utils.h:18:3: note: in expansion of macro ‘CHECK_CUDA’\n     18 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp:18:3: note: in expansion of macro ‘CHECK_INPUT’\n     18 |   CHECK_INPUT(p_choose);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.h:11,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp:9:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/core/Device.h:5,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.h:11,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp:9:\n  /kaggle/working/fairseq/examples/operators/utils.h:14:21: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     14 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/macros/Macros.h:223:64: note: in definition of macro ‘C10_UNLIKELY’\n    223 | #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n        |                                                                ^~~~\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/c10/util/Exception.h:534:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n    534 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n        |       ^~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/utils.h:14:3: note: in expansion of macro ‘TORCH_CHECK’\n     14 |   TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n        |   ^~~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/utils.h:18:3: note: in expansion of macro ‘CHECK_CUDA’\n     18 |   CHECK_CUDA(x);       \\\n        |   ^~~~~~~~~~\n  /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp:19:3: note: in expansion of macro ‘CHECK_INPUT’\n     19 |   CHECK_INPUT(alpha);\n        |   ^~~~~~~~~~~\n  In file included from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.h:11,\n                   from /kaggle/working/fairseq/examples/operators/alignment_train_cuda.cpp:9:\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n    225 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_kernel.o.d -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c -c /kaggle/working/fairseq/examples/operators/alignment_train_kernel.cu -o /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=alignment_train_cuda_binding -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_cuda.o /tmp/tmpeofm26wo.build-temp/examples/operators/alignment_train_kernel.o -L/tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o /tmp/tmp3yke0n0e.build-lib/alignment_train_cuda_binding.cpython-310-x86_64-linux-gnu.so\n  copying /tmp/tmp3yke0n0e.build-lib/fairseq/libbleu.cpython-310-x86_64-linux-gnu.so -> fairseq\n  copying /tmp/tmp3yke0n0e.build-lib/fairseq/data/data_utils_fast.cpython-310-x86_64-linux-gnu.so -> fairseq/data\n  copying /tmp/tmp3yke0n0e.build-lib/fairseq/data/token_block_utils_fast.cpython-310-x86_64-linux-gnu.so -> fairseq/data\n  copying /tmp/tmp3yke0n0e.build-lib/fairseq/libbase.cpython-310-x86_64-linux-gnu.so -> fairseq\n  copying /tmp/tmp3yke0n0e.build-lib/fairseq/libnat.cpython-310-x86_64-linux-gnu.so -> fairseq\n  copying /tmp/tmp3yke0n0e.build-lib/alignment_train_cpu_binding.cpython-310-x86_64-linux-gnu.so ->\n  copying /tmp/tmp3yke0n0e.build-lib/fairseq/libnat_cuda.cpython-310-x86_64-linux-gnu.so -> fairseq\n  copying /tmp/tmp3yke0n0e.build-lib/fairseq/ngram_repeat_block_cuda.cpython-310-x86_64-linux-gnu.so -> fairseq\n  copying /tmp/tmp3yke0n0e.build-lib/alignment_train_cuda_binding.cpython-310-x86_64-linux-gnu.so ->\n  Editable install will be performed using a meta path finder.\n\n  Options like `package-data`, `include/exclude-package-data` or\n  `packages.find.exclude/include` may have no effect.\n\n  adding '__editable___fairseq_0_12_2_finder.py'\n  adding '__editable__.fairseq-0.12.2.pth'\n  creating '/tmp/pip-wheel-tpeqfl8z/.tmp-1p5ni7zx/fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl' and adding '/tmp/tmpkaee4i8efairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl' to it\n  adding 'fairseq-0.12.2.dist-info/LICENSE'\n  adding 'fairseq-0.12.2.dist-info/METADATA'\n  adding 'fairseq-0.12.2.dist-info/WHEEL'\n  adding 'fairseq-0.12.2.dist-info/entry_points.txt'\n  adding 'fairseq-0.12.2.dist-info/top_level.txt'\n  adding 'fairseq-0.12.2.dist-info/RECORD'\n  /tmp/pip-build-env-c0rz84oa/overlay/local/lib/python3.10/dist-packages/setuptools/command/editable_wheel.py:342: InformationOnly: Editable installation.\n  !!\n\n          ********************************************************************************\n          Please be careful with folders in your working directory with the same\n          name as your package as they may take precedence during imports.\n          ********************************************************************************\n\n  !!\n    with strategy, WheelFile(wheel_path, \"w\") as wheel_obj:\n  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9579 sha256=3c6e6965e9a0be836ab891817f503719e7e2dca057ec7def67f1a3b8354a91e1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-er52fuso/wheels/67/69/06/aa788a7d879a7513f5f2e5c6d56825b6813783cb55ac5c7f8d\nSuccessfully built fairseq\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: bitarray, portalocker, sacrebleu, fairseq\n  changing mode of /usr/local/bin/sacrebleu to 755\n  changing mode of /usr/local/bin/fairseq-eval-lm to 755\n  changing mode of /usr/local/bin/fairseq-generate to 755\n  changing mode of /usr/local/bin/fairseq-hydra-train to 755\n  changing mode of /usr/local/bin/fairseq-interactive to 755\n  changing mode of /usr/local/bin/fairseq-preprocess to 755\n  changing mode of /usr/local/bin/fairseq-score to 755\n  changing mode of /usr/local/bin/fairseq-train to 755\n  changing mode of /usr/local/bin/fairseq-validate to 755\nSuccessfully installed bitarray-3.0.0 fairseq-0.12.2 portalocker-3.0.0 sacrebleu-2.4.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:10.820809Z","iopub.execute_input":"2024-12-26T20:42:10.821106Z","iopub.status.idle":"2024-12-26T20:42:20.840721Z","shell.execute_reply.started":"2024-12-26T20:42:10.821079Z","shell.execute_reply":"2024-12-26T20:42:20.839794Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.0.5-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.5 rapidfuzz-3.11.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import jiwer\nimport fairseq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:27.330198Z","iopub.execute_input":"2024-12-26T20:42:27.330517Z","iopub.status.idle":"2024-12-26T20:42:27.356234Z","shell.execute_reply.started":"2024-12-26T20:42:27.330494Z","shell.execute_reply":"2024-12-26T20:42:27.355405Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Necessary Library Imports\nimport torch\nimport torchaudio\nimport torch.utils.data\nimport json\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom typing import List, Dict\n\n# For Trainer\nimport os\n# import wandb\nfrom pathlib import Path\nfrom glob import glob\nfrom torch import optim, nn, Tensor\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom abc import ABC\nfrom torchmetrics import WordErrorRate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:30.086065Z","iopub.execute_input":"2024-12-26T20:42:30.086363Z","iopub.status.idle":"2024-12-26T20:42:35.983630Z","shell.execute_reply.started":"2024-12-26T20:42:30.086340Z","shell.execute_reply":"2024-12-26T20:42:35.982943Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## **Audio Dataset**","metadata":{}},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, label_manifest_path: str, unlabel_manifest_path: str = None):\n        super().__init__()\n        self.list_manifest = json.load(open(label_manifest_path, \"r\"))\n        if unlabel_manifest_path:\n            self.list_manifest += json.load(open(unlabel_manifest_path, \"r\"))\n\n    def __len__(self):\n        return len(self.list_manifest)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        return:\n        - for label example: wav, sr, text\n        - for unlabel example: wav, sr, None\n        \"\"\"\n        # label: { 'audio_filepath': ..., 'text': \"hello there\" }\n        # unlabel: { 'audio_filepath': ...}\n        sample = self.list_manifest[idx]\n        wav, sr = torchaudio.load(sample.get(\"audio_filepath\"))\n        return dict(wav=wav, text=sample.get(\"text\", None))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:35.984493Z","iopub.execute_input":"2024-12-26T20:42:35.984865Z","iopub.status.idle":"2024-12-26T20:42:35.989911Z","shell.execute_reply.started":"2024-12-26T20:42:35.984831Z","shell.execute_reply":"2024-12-26T20:42:35.989296Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## **Utils**","metadata":{}},{"cell_type":"code","source":"test_subset = [\"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"]\n\n\ndef calc_length(lengths, padding, kernel_size, stride, ceil_mode, repeat_num=1):\n    \"\"\"Calculates the output length of a Tensor passed through a convolution or max pooling layer\"\"\"\n    add_pad: float = (padding * 2) - kernel_size\n    one: float = 1.0\n    for i in range(repeat_num):\n        lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one\n        if ceil_mode:\n            lengths = torch.ceil(lengths)\n        else:\n            lengths = torch.floor(lengths)\n    return lengths.to(dtype=torch.int)\n\n\nclass ConvSubsampling(nn.Module):\n    \"\"\"Convolutional subsampling which supports VGGNet and striding approach introduced in:\n    VGGNet Subsampling: https://arxiv.org/pdf/1910.12977.pdf\n    Striding Subsampling:\n        \"Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition\" by Linhao Dong et al.\n    Args:\n        input_dim (int): size of the input features\n        feat_out (int): size of the output features\n        conv_channels (int): Number of channels for the convolution layers. (encoder dim)\n        subsampling_factor (int): The subsampling factor which should be a power of 2\n        activation (Module): activation function, default is nn.ReLU()\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 80,\n        feat_out: int = -1,\n        conv_channels: int = -1,\n        subsampling_factor: int = 4,\n        activation=nn.ReLU(),\n    ):\n        super(ConvSubsampling, self).__init__()\n\n        if subsampling_factor % 2 != 0:\n            raise ValueError(\"Sampling factor should be a multiply of 2!\")\n        self._sampling_num = int(math.log(subsampling_factor, 2))\n\n        in_channels = 1\n        layers = []\n\n        self._padding = 1\n        self._stride = 2\n        self._kernel_size = 3\n        self._ceil_mode = False\n\n        for i in range(self._sampling_num):\n            layers.append(\n                torch.nn.Conv2d(\n                    in_channels=in_channels,\n                    out_channels=conv_channels,\n                    kernel_size=self._kernel_size,\n                    stride=self._stride,\n                    padding=self._padding,\n                )\n            )\n            layers.append(activation)\n            in_channels = conv_channels\n\n        in_length = torch.tensor(input_dim, dtype=torch.float)\n        out_length = calc_length(\n            in_length,\n            padding=self._padding,\n            kernel_size=self._kernel_size,\n            stride=self._stride,\n            ceil_mode=self._ceil_mode,\n            repeat_num=self._sampling_num,\n        )\n        self.out = torch.nn.Linear(conv_channels * int(out_length), feat_out)\n        self.conv = torch.nn.Sequential(*layers)\n\n    def forward(self, x, lengths):\n        lengths = calc_length(\n            lengths,\n            padding=self._padding,\n            kernel_size=self._kernel_size,\n            stride=self._stride,\n            ceil_mode=self._ceil_mode,\n            repeat_num=self._sampling_num,\n        )\n        x = x.unsqueeze(1)\n        x = self.conv(x)\n        b, c, t, f = x.size()\n        x = self.out(x.transpose(1, 2).reshape(b, t, -1))\n        return x, lengths\n\n\nclass LogMelSpectrogram(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n            sample_rate=16000, **kwargs\n        )\n\n    def forward(self, inputs):\n        return self.mel_spec(inputs)\n\n\nclass ComposeTransform:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, audio_data):\n        for t in self.transforms:\n            audio_data = t(audio_data)\n        return audio_data\n\n\ndef count_params(model):\n    if type(model) == nn.DataParallel:\n        return model.module.count_params()\n    return model.count_params()\n\n\ndef save_state_dict(model):\n    if type(model) == nn.DataParallel:\n        return model.module.state_dict()\n    return model.state_dict()\n\n\nclass EarlyStopping:\n    def __init__(self, tolerance=5, min_delta=0):\n\n        self.tolerance = tolerance\n        self.min_delta = min_delta\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, train_loss, validation_loss):\n        if (validation_loss - train_loss) > self.min_delta:\n            self.counter += 1\n            if self.counter >= self.tolerance:\n                self.early_stop = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:35.991520Z","iopub.execute_input":"2024-12-26T20:42:35.991735Z","iopub.status.idle":"2024-12-26T20:42:36.008593Z","shell.execute_reply.started":"2024-12-26T20:42:35.991717Z","shell.execute_reply":"2024-12-26T20:42:36.008008Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## **Dataset (LibriLight)**","metadata":{}},{"cell_type":"code","source":"import torchaudio\n#from utils import LogMelSpectrogram\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport torch\nfrom typing import List\n\n\nclass LibriLight(Dataset):\n    def __init__(\n        self,\n        data_path: str = \"/kaggle/input/libri-phone/libri-phone\",\n        subsets: List[str] = [\"light\", \"dev-clean\", \"dev-other\"],\n        n_fft: int = 2048,\n        n_mels: int = 80,\n        win_length: int = 400,\n        hop_length: int = 100,\n    ):\n        super().__init__()\n        # assert subset in ['light', 'dev-clean', 'dev-other', 'test-clean', 'test-other'], 'Not found subset'\n        df = pd.read_csv(data_path + os.sep + \"phones.csv\")\n        df = df[df.subset.isin(subsets)].drop(\"subset\", axis=1)\n        df.label = df.label.map(eval)\n        df.path = data_path + os.sep + df.path\n        df.path = df.path.apply(lambda x: x.replace(\"\\\\\", os.sep))\n        self.walker = df.to_dict(\"records\")\n\n        self.feature_transform = LogMelSpectrogram(\n            n_fft=n_fft, n_mels=n_mels, win_length=win_length, hop_length=hop_length\n        )\n\n    def __len__(self):\n        return len(self.walker)\n\n    def __getitem__(self, idx):\n        item = self.walker[idx]\n        label = item[\"label\"]\n        wave, sr = torchaudio.load(item[\"path\"])\n\n        specs = self.feature_transform(wave)\n        specs = specs.permute(0, 2, 1)\n        specs = specs.squeeze()\n\n        return specs, label\n\n\nclass LibriLightLibriSpeechDataset(Dataset):\n    def __init__(\n        self,\n        light_data_path: str = \"data/libri-phone\",\n        subset: str = \"train\",\n        libri_clean_path: str = \"data/LibriSpeech\",\n        n_fft: int = 1024,\n        n_mels: int = 128,\n        win_length: int = 400,\n        hop_length: int = 200,\n        **kwargs,\n    ):\n        super().__init__()\n        \"\"\"\n        subset \\in ['train', 'val', 'test']\n        \"\"\"\n        self.list_url = []\n        is_test = True\n        if subset == \"train\":\n            self.list_url = [libri_clean_path + \"/train-clean-360\"]\n            is_test = False\n\n        sep = os.sep\n        self.libri_walker = []\n        for path in self.list_url:\n            files_path = f\"*{sep}*{sep}*\" + \".flac\"\n            walker = [(str(p.stem), path) for p in Path(path).glob(files_path)]\n            self.libri_walker.extend(walker)\n\n        if subset == \"train\":\n            subsets = [\"light\", \"dev-clean\", \"dev-other\", \"test-other\"]\n        else:\n            subsets = [\"test-clean\"]\n\n        df = pd.read_csv(light_data_path + os.sep + \"phones.csv\")\n        df = df[df.subset.isin(subsets)].drop(\"subset\", axis=1)\n        df.label = df.label.map(eval)\n        df.path = light_data_path + os.sep + df.path\n        df.path = df.path.apply(lambda x: x.replace(\"\\\\\", os.sep))\n        self.light_walker = df.to_dict(\"records\")\n\n        self.walker = self.libri_walker + self.light_walker\n\n        sample_rate = 16000\n        self.feature_transform = LogMelSpectrogram(\n            n_fft=n_fft, n_mels=n_mels, win_length=win_length, hop_length=hop_length\n        )\n\n    def __len__(self):\n        return len(self.walker)\n\n    def __getitem__(self, idx):\n        item = self.walker[idx]\n        if type(item) == tuple:\n            return self.load_librispeech_item(item)\n        return self.load_libri_light_item(item)\n\n    def load_libri_light_item(self, item):\n        label = item[\"label\"]\n        wave, sr = torchaudio.load(item[\"path\"])\n\n        specs = self.feature_transform(wave)\n        specs = specs.permute(0, 2, 1)\n        specs = specs.squeeze()\n\n        return specs, label, \"labeled\"\n\n    def load_librispeech_item(self, item):\n        \"\"\"\n        transform audio pack to spectrogram\n        \"\"\"\n        fileid, path = item\n\n        speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n        fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n        file_audio = fileid_audio + \".flac\"\n        file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n\n        # Load audio\n        waveform, sample_rate = torchaudio.load(file_audio)\n\n        spectrogram = self.feature_transform(waveform)\n        spectrogram = spectrogram.squeeze().permute(1, 0)\n\n        return spectrogram, \"unlabel\"\n\n\n# class TimitLibriSpeechDataset(Dataset):\n#     def __init__(\n#         self,\n#         timit_data_root: str = 'data/timit',\n#         timit_csv_path: str = 'data/timit/timit_pronunciation.csv',\n#         libri_clean_path: str = 'data/libri/LibriSpeech',\n#         libri_other_path: str = '',\n#         subset: str = 'train',\n#         n_fft: int = 512,\n#         n_mels: int = 80,\n#         **kwargs,\n#     ):\n#         super().__init__()\n#         \"\"\"\n#         subset \\in ['train', 'val', 'test']\n#         \"\"\"\n#         self.list_url = []\n#         is_test = True\n#         if subset == \"train\":\n#             self.list_url = [libri_clean_path + \"train-clean-100\"]\n#             is_test = False\n\n#         sep = os.sep\n#         self.libri_walker = []\n#         for path in self.list_url:\n#             files_path = f\"*{sep}*{sep}*\" + '.flac'\n#             walker = [(str(p.stem), path) for p in Path(path).glob(files_path)]\n#             self.libri_walker.extend(walker)\n\n#         df = pd.read_csv(timit_csv_path, index_col=0)\n#         df = df[df.is_test == is_test].drop(\"is_test\", axis=1)\n#         df.path = df.path.apply(lambda x: timit_data_root + os.sep + x)\n#         df.trans = df.trans.str.split(\"|\")\n#         self.is_test = is_test\n\n#         self.timit_walker = df.to_dict(\"records\")\n\n#         self.walker = self.libri_walker + self.timit_walker\n\n#         sample_rate = 16000\n#         self.feature_transform = LogMelSpectrogram(n_fft=n_fft, n_mels=n_mels)\n#         self.augmentation = ComposeTransform([\n#             SpeedPerturbation(sample_rate),\n#             RandomBackgroundNoise(sample_rate, max_snr_db=30)\n#         ])\n#         self.augment_prob = 0.80\n\n\n#     def __len__(self):\n#         return len(self.walker)\n\n#     def __getitem__(self, idx):\n#         item = self.walker[idx]\n#         if type(item) == tuple:\n#             return self.load_librispeech_item(item)\n#         return self.load_timit_item(item)\n\n#     def load_timit_item(self, item):\n#         trans = item[\"trans\"]\n#         wave, sr = torchaudio.load(item[\"path\"])\n\n#         is_augment = np.random.choice(2, p=(1 - self.augment_prob, self.augment_prob))\n#         if is_augment and not self.is_test:\n#             wave = self.augmentation(wave)\n\n#         specs = self.feature_transform(wave)\n#         specs = specs.permute(0, 2, 1)\n#         specs = specs.squeeze()\n#         return specs, trans, 'labelled'\n\n#     def load_librispeech_item(self, item):\n#         \"\"\"\n#         transform audio pack to spectrogram\n#         \"\"\"\n#         fileid, path = item\n\n#         speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n#         fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n#         file_audio = fileid_audio + '.flac'\n#         file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n\n#         # Load audio\n#         waveform, sample_rate = torchaudio.load(file_audio)\n\n#         is_augment = np.random.choice(2, p=(1 - self.augment_prob, self.augment_prob))\n#         if is_augment:\n#             waveform = self.augmentation(waveform)\n\n#         spectrogram = self.feature_transform(waveform)\n#         spectrogram = spectrogram.squeeze().permute(1, 0)\n\n#         return spectrogram, 'unlabel'\n\n\nclass TimitLibriSpeechDataset(Dataset):\n    def __init__(\n        self,\n        timit_data_root: str = \"/kaggle/input/libri-phone/libri-phone\",\n        timit_csv_path: str = \"/kaggle/input/libri-phone/libri-phone/phones.csv\",\n        libri_clean_path: str = \"data/libri/LibriSpeech\",\n        libri_other_path: str = \"\",\n        subset: str = \"train\",\n        n_fft: int = 512,\n        n_mels: int = 80,\n        **kwargs,\n    ):\n        super().__init__()\n        \"\"\"\n        subset \\in ['train', 'val', 'test']\n        \"\"\"\n        self.list_url = []\n        is_test = True\n        if subset == \"train\":\n            self.list_url = [libri_clean_path + \"train-clean-100\"]\n            is_test = False\n\n        sep = os.sep\n        self.libri_walker = []\n        for path in self.list_url:\n            files_path = f\"*{sep}*{sep}*\" + \".flac\"\n            walker = [(str(p.stem), path) for p in Path(path).glob(files_path)]\n            self.libri_walker.extend(walker)\n\n        df = pd.read_csv(timit_csv_path, index_col=0)\n        df = df[df.is_test == is_test].drop(\"is_test\", axis=1)\n        df.path = df.path.apply(lambda x: timit_data_root + os.sep + x)\n        df.trans = df.trans.str.split(\"|\")\n        self.is_test = is_test\n\n        self.timit_walker = df.to_dict(\"records\")\n\n        self.walker = self.libri_walker + self.timit_walker\n\n        sample_rate = 16000\n        self.feature_transform = LogMelSpectrogram(n_fft=n_fft, n_mels=n_mels)\n\n    def __len__(self):\n        return len(self.walker)\n\n    def __getitem__(self, idx):\n        item = self.walker[idx]\n        if type(item) == tuple:\n            return self.load_librispeech_item(item)\n        return self.load_timit_item(item)\n\n    def load_timit_item(self, item):\n        trans = item[\"trans\"]\n        wave, sr = torchaudio.load(item[\"path\"])\n\n        specs = self.feature_transform(wave)\n        specs = specs.permute(0, 2, 1)\n        specs = specs.squeeze()\n        return specs, trans, \"labelled\"\n\n    def load_librispeech_item(self, item):\n        \"\"\"\n        transform audio pack to spectrogram\n        \"\"\"\n        fileid, path = item\n\n        speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n        fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n        file_audio = fileid_audio + \".flac\"\n        file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n\n        # Load audio\n        waveform, sample_rate = torchaudio.load(file_audio)\n\n        spectrogram = self.feature_transform(waveform)\n        spectrogram = spectrogram.squeeze().permute(1, 0)\n\n        return spectrogram, \"unlabel\"\n\n\nclass TimitDataset(Dataset):\n    def __init__(\n        self,\n        data_root: str = \"data/timit\",\n        csv_path: str = \"data/timit/timit_pronunciation.csv\",\n        n_fft: int = 159,\n        n_mels: int = 80,\n        is_test: bool = False,\n        **kwargs,\n    ):\n        super().__init__()\n        df = pd.read_csv(csv_path, index_col=0)\n        df = df[df.is_test == is_test].drop(\"is_test\", axis=1)\n        df.path = df.path.apply(lambda x: data_root + os.sep + x)\n        df.trans = df.trans.str.split(\"|\")\n        self.is_test = is_test\n        sample_rate = 16000\n\n        self.walker = df.to_dict(\"records\")\n        self.feature_transform = LogMelSpectrogram(n_fft=n_fft, n_mels=n_mels)\n\n    def __len__(self):\n        return len(self.walker)\n\n    def __getitem__(self, idx):\n        item = self.walker[idx]\n        trans = item[\"trans\"]\n        wave, sr = torchaudio.load(item[\"path\"])\n\n        specs = self.feature_transform(wave)\n\n        specs = specs.permute(0, 2, 1)\n        specs = specs.squeeze()\n\n        return specs, trans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:36.009420Z","iopub.execute_input":"2024-12-26T20:42:36.009651Z","iopub.status.idle":"2024-12-26T20:42:36.446361Z","shell.execute_reply.started":"2024-12-26T20:42:36.009633Z","shell.execute_reply":"2024-12-26T20:42:36.445678Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **DataLoader**","metadata":{}},{"cell_type":"code","source":"# from dataloader.dataset import AudioDataset\n\nfrom torch.utils.data import DataLoader\nfrom functools import partial\n\n\ndef collate_fn(batch, audio_transform, text_process):\n    transform_fn = lambda x: audio_transform(x).squeeze().permute(1, 0)\n    text_transform_fn = lambda x: text_process.text2int(text_process.tokenize(x))\n\n    wavs = [i[\"wav\"] for i in batch]\n    feat = list(map(transform_fn, wavs))\n    feat_len = torch.LongTensor([i.size(0) for i in feat])\n    feat = pad_sequene(feat, batch_first=True)\n\n    trans = [\n        (idx, item[\"text\"])\n        for idx, text in enumerate(batch)\n        if item[\"text\"] is not None\n    ]\n\n    if len(trans) == len(batch):\n        target = list(map(text_transform_fn, [i[1] for i in trans]))\n        target_len = torch.LongTensor([i.size(0) for i in target])\n        target = pad_sequence(target, batch_first=True)\n\n        # for all label examples\n        return feat, feat_len, target, target_len\n\n    return feat, feat_len, trans\n\n\ndef create_dataloader(dataset_cfg: dict, dataloader_cfg: dict, text_process):\n\n    audio_transform = MelSpectrogram(**dataset_cfg.mel_spectrogram_cfg)\n\n    fused_collate_fn = partial(\n        collate_fn, text_process=text_process, audio_transform=audio_transform\n    )\n\n    train_loader = DataLoader(\n        AudioDataset(\n            label_manifest_path=dataset_cfg.manifest_path.train.label,\n            unlabel_manifest_path=dataset_cfg.manifest_path.train.unlabel,\n        ),\n        shuffle=True,\n        collate_fn=fused_collate_fn,\n        **dataloader_cfg\n    )\n\n    valid_loader = DataLoader(\n        AudioDataset(\n            label_manifest_path=dataset_cfg.manifest_path.train.valid,\n        ),\n        shuffle=False,\n        collate_fn=fused_collate_fn,\n        **dataloader_cfg\n    )\n\n    test_loader = DataLoader(\n        AudioDataset(\n            label_manifest_path=dataset_cfg.manifest_path.train.test,\n        ),\n        shuffle=False,\n        collate_fn=fused_collate_fn,\n        **dataloader_cfg\n    )\n\n    predict_loader = DataLoader(\n        AudioDataset(\n            label_manifest_path=dataset_cfg.manifest_path.train.predict,\n        ),\n        shuffle=False,\n        collate_fn=fused_collate_fn,\n        **dataloader_cfg\n    )\n\n    return train_loader, valid_loader, test_loader, predict_loader\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:36.447102Z","iopub.execute_input":"2024-12-26T20:42:36.447462Z","iopub.status.idle":"2024-12-26T20:42:36.456129Z","shell.execute_reply.started":"2024-12-26T20:42:36.447438Z","shell.execute_reply":"2024-12-26T20:42:36.455293Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# **Training Model**","metadata":{}},{"cell_type":"code","source":"import os\n# import wandb\nfrom pathlib import Path\nfrom glob import glob\nfrom torch import optim, nn, Tensor\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom abc import ABC\nfrom torchmetrics import WordErrorRate\n\n\nclass Trainer(ABC):\n    def __init__(\n        self,\n        max_epochs: int,\n        experiment_path: str,\n        wandb_conf: dict,\n        optim_conf: dict,\n        scheduler_conf: dict,\n        text_process,\n        device: str = \"cpu\",\n    ):\n        super().__init__()\n        self.device = device\n        self.optim_conf = optim_conf\n        self.scheduler_conf = scheduler_conf\n        self.max_epochs = max_epochs\n        self.wandb_conf = wandb_conf\n        self.experiment_path = experiment_path\n\n        if os.path.exists(experiment_path):\n            os.mkdir(experiment_path)\n\n        if wandb_conf.is_log:\n            wandb.init(**wandb_conf.config)\n\n    def get_optimizer_and_scheduler(self, model: nn.Module, dataloader: DataLoader):\n        optimizer = getattr(optim, self.optim_conf.optim_name)(\n            model.paramters(), **self.optim_conf\n        )\n        if self.scheduler_conf.sched_name == \"OneCycleLR\":\n            # for training only\n            self.scheduler_conf.update(\n                {\"total_steps\": len(dataloader) * self.max_epochs}\n            )\n        scheduler = getattr(optim.lr_scheduler, self.scheduler_conf.sched_name)(\n            optimizer, **self.scheduler_conf\n        )\n\n        if self.optim_ckpt:\n            optimizer.load_state_dict(self.optim_ckpt.get(\"optimizer_state_dict\"))\n            scheduler.load_state_dict(self.optim_ckpt.get(\"scheduler_state_dict\"))\n\n        return optimizer, scheduler\n\n    def save_ckpt(\n        self,\n        model: nn.Module,\n        optimizer: Optimizer,\n        scheduler: _LRScheduler,\n        epoch: int = -1,\n        step: int = -1,\n    ) -> None:\n        trainer = dict(\n            optim=dict(\n                optimizer_state_dict=optimizer.state_dict(),\n                scheduler_state_dict=scheduler.state_dict(),\n            ),\n            hyperparams=self.__dict__,\n        )\n\n        model = dict(model_state_dict=model.state_dict(), hyperparams=model.__dict__)\n\n        version = len(glob(str(Path(self.experiment_path) / \"version_*\")))\n        version_path = os.path.join(self.experiment_path, f\"version_{version}\")\n        os.mkdir(version_path)\n\n        trainer_name = f\"{self.__class__.__name__}.epoch={epoch}.step={step}.pt\"\n        model_name = f\"{model.__class__.__name__}.epoch={epoch}.step={step}.pt\"\n\n        trainer_path = os.path.join(version_path, trainer_name)\n        model_path = os.path.join(version_path, model_name)\n\n        torch.save(trainer, trainer_path)\n        torch.save(model, model_path)\n\n    def load_from_ckpt(self, ckpt_path: dict) -> None:\n        trainer_ckpt_path = ckpt_path.get(\"trainer\")\n\n        if trainer_ckpt_path:\n            trainer = torch.load(trainer_ckpt_path)\n            for k, v in trainer.get(\"hyperparams\"):\n                setattr(self, k, v)\n\n        print(\"<Restore Trainer checkpoint successfully>\")\n\n    def train(self, model: nn.Module, dataloader: DataLoader):\n        optimizer, scheduler = self.get_optimizer_and_scheduler(model, dataloader)\n\n        for epoch in range(1, self.max_epochs + 1):\n            self.train_epoch(model, dataloader, optimizer, scheduler, epoch)\n            self.test_epoch(model, dataloader, epoch, \"valid\")\n\n            if self.scheduler_conf.interval == \"epoch\":\n                scheduler.step()\n\n    def test(self, model: nn.Module, dataloader: DataLoader):\n        self.test_epoch(model, dataloader, 0, \"test\")\n\n    def predict(self, model: nn.Module, dataloader: DataLoader, outcome_path: str):\n        self.test_epoch(model, dataloader, 0, outcome_path)\n\n    def train_epoch(\n        self,\n        model: nn.Module,\n        dataloader: DataLoader,\n        optimizer: Optimizer,\n        scheduler: _LRScheduler,\n        epoch: int,\n    ):\n        pass\n\n    def test_epoch(\n        self,\n        model: nn.Module,\n        dataloader: DataLoader,\n        epoch: int,\n        task: str = \"test\",\n        outcome_name: str = None,\n    ):\n        pass\n\n\nclass TeacherTrainer(Trainer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def train_epoch(\n        self,\n        model: nn.Module,\n        dataloader: DataLoader,\n        optimizer: Optimizer,\n        scheduler: _LRScheduler,\n        epoch: int,\n    ):\n        size = len(dataloader)\n        pbar = tqdm(dataloader, total=size)\n\n        for batch_idx, batch in enumerate(tqdm, start=1):\n            feat, feat_len, target, target_len = list(\n                map(lambda x: x.to(self.device), batch)\n            )\n\n            optimizer.zero_grad()\n\n            # for training only\n            out, out_len, loss = model(feat, feat_len, target, target_len)\n\n            loss.backward()\n\n            optimizer.step()\n\n            if self.scheduler_conf.interval == \"step\":\n                scheduler.step()\n\n            # if self.wandb_conf.is_log:\n            #     wandb.log({\"train/loss\": loss.item()})\n\n            #     sched_name = scheduler.__class__.__name__\n            #     last_lr = scheduler.get_last_lr()[0]\n            #     wandb.log({f\"lr-{sched_name}\": last_lr})\n\n            pbar.set_description(f\"[Epoch: {epoch}] Loss: {loss.item():.2f}\")\n\n            self.save_ckpt(model, optimizer, scheduler, epoch, epoch * batch_idx)\n\n    def test_epoch(\n        self,\n        model: nn.Module,\n        dataloader: DataLoader,\n        epoch: int,\n        task: str = \"test\",\n        outcome_name: str = None,\n    ):\n        size = len(dataloader)\n        pbar = tqdm(dataloader, total=size)\n        cal_wer = WordErrorRate()\n\n        outcome_path = os.path.join(self.experiment_path, outcome_name)\n\n        with open(outcome_path, \"a\") as f:\n            f.write(\"=\" * 10 + f\"{task} | Epoch: {epoch}\" + \"=\" * 10)\n            f.write(\"\\n\")\n\n        with torch.inference_mode():\n            for batch_idx, batch in enumerate(tqdm, start=1):\n                feat, feat_len, target, target_len = list(\n                    map(lambda x: x.to(self.device), batch)\n                )\n\n                # for training only\n                out, out_len, loss = model(\n                    feat, feat_len, target, target_len, predict=True\n                )\n\n                predict = model.recognize(inputs, input_lengths)\n                actual = list(map(self.text_process.int2text, targets))\n                list_wer = [\n                    cal_wer(hypot, truth).item()\n                    for hypot, truth in zip(predict, actual)\n                ]\n                mean_wer = cal_wer(predict, actual).item()\n\n                # with open(outcome_path, \"a\") as f:\n                #     for pred, act, wer in zip(predict, actual, list_wer):\n                #         f.write(f\"PER    : {wer}\\n\")\n                #         f.write(f\"Actual : {act}\\n\")\n                #         f.write(f\"Predict: {pred}\\n\")\n                #         f.write(\"=\" * 20 + \"\\n\")\n\n                # if self.wandb_conf.is_log:\n                #     wandb.log({f\"{task}/loss\": loss.item()})\n                #     wandb.log({f\"{task}/wer\": mean_wer})\n\n                pbar.set_description(\n                    f\"[Epoch: {epoch}] Loss: {loss.item():.2f} | WER: {mean_wer:.2f}%\"\n                )\n\n\nclass StudentTrainer(Trainer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def train_epoch(\n        self,\n        teacher_model: nn.Module,\n        student_model: nn.Module,\n        dataloader: DataLoader,\n        optimizer: Optimizer,\n        scheduler: _LRScheduler,\n        epoch: int,\n    ):\n        size = len(dataloader)\n        pbar = tqdm(dataloader, total=size)\n\n        for batch_idx, batch in enumerate(tqdm, start=1):\n            feat, feat_len, trans = batch\n            feat, feat_len = feat.to(self.device), feat_len.to(self.device)\n\n            # teacher generate pseudo-label for student learning\n            predicted = teacher_model.recognize(inputs, input_lengths)\n\n            # replace the origin transcript of timit dataset\n            for origin_trans, idx in trans:\n                predicted[idx] = origin_trans\n\n            for i in range(len(predicted)):\n                if type(predicted[i]) == str:\n                    predicted[i] = self.text_process.tokenize(predicted[i])\n\n            predicted = [self.text_process.text2int(s) for s in predicted]\n\n            target_len = torch.IntTensor([s.size(0) for s in predicted]).to(self.device)\n            target = pad_sequence(predicted, batch_first=True).to(\n                self.device, torch.int\n            )\n\n            optimizer.zero_grad()\n\n            # for training only\n            out, out_len, loss = student_model(feat, feat_len, target, target_len)\n\n            loss.backward()\n\n            optimizer.step()\n\n            if self.scheduler_conf.interval == \"step\":\n                scheduler.step()\n\n            # if self.wandb_conf.is_log:\n            #     wandb.log({\"train/loss\": loss.item()})\n\n            #     sched_name = scheduler.__class__.__name__\n            #     last_lr = scheduler.get_last_lr()[0]\n            #     wandb.log({f\"lr-{sched_name}\": last_lr})\n\n            pbar.set_description(f\"[Epoch: {epoch}] Loss: {loss.item():.2f}\")\n\n            self.save_ckpt(\n                student_model, optimizer, scheduler, epoch, epoch * batch_idx\n            )\n\n    def test_epoch(\n        self,\n        student_model: nn.Module,\n        dataloader: DataLoader,\n        epoch: int,\n        outcome_name: str = None,\n    ):\n        size = len(dataloader)\n        pbar = tqdm(dataloader, total=size)\n        cal_wer = WordErrorRate()\n\n        outcome_path = os.path.join(self.experiment_path, outcome_name)\n\n        with open(outcome_path, \"a\") as f:\n            f.write(\"=\" * 10 + f\"{task} | Epoch: {epoch}\" + \"=\" * 10)\n            f.write(\"\\n\")\n\n        with torch.inference_mode():\n            for batch_idx, batch in enumerate(tqdm, start=1):\n                feat, feat_len, target, target_len = list(\n                    map(lambda x: x.to(self.device), batch)\n                )\n\n                # for training only\n                out, out_len, loss = student_model(\n                    feat, feat_len, target, target_len, predict=True\n                )\n\n                predict = student_model.recognize(inputs, input_lengths)\n                actual = list(map(self.text_process.int2text, targets))\n                list_wer = [\n                    cal_wer(hypot, truth).item()\n                    for hypot, truth in zip(predict, actual)\n                ]\n                mean_wer = cal_wer(predict, actual).item()\n\n                # with open(outcome_path, \"a\") as f:\n                #     for pred, act, wer in zip(predict, actual, list_wer):\n                #         f.write(f\"PER    : {wer}\\n\")\n                #         f.write(f\"Actual : {act}\\n\")\n                #         f.write(f\"Predict: {pred}\\n\")\n                #         f.write(\"=\" * 20 + \"\\n\")\n\n                # if self.wandb_conf.is_log:\n                #     wandb.log({f\"{task}/loss\": loss.item()})\n                #     wandb.log({f\"{task}/wer\": mean_wer})\n\n                pbar.set_description(\n                    f\"[Epoch: {epoch}] Loss: {loss.item():.2f} | WER: {mean_wer:.2f}%\"\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:42:36.456897Z","iopub.execute_input":"2024-12-26T20:42:36.457200Z","iopub.status.idle":"2024-12-26T20:42:36.483686Z","shell.execute_reply.started":"2024-12-26T20:42:36.457178Z","shell.execute_reply":"2024-12-26T20:42:36.482912Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## **Convolutional Subsampling**","metadata":{}},{"cell_type":"code","source":"class ConvSubsampling(nn.Module):\n    def __init__(\n        self,\n        input_dim: int = 80,\n        feat_out: int = -1,\n        conv_channels: int = -1,\n        subsampling_factor: int = 4,\n        activation=nn.ReLU(),\n    ):\n        super(ConvSubsampling, self).__init__()\n\n        if subsampling_factor % 2 != 0:\n            raise ValueError(\"Sampling factor should be a multiply of 2!\")\n        self._sampling_num = int(math.log(subsampling_factor, 2))\n\n        in_channels = 1\n        layers = []\n\n        self._padding = 1\n        self._stride = 2\n        self._kernel_size = 3\n        self._ceil_mode = False\n\n        for i in range(self._sampling_num):\n            layers.append(\n                torch.nn.Conv2d(\n                    in_channels=in_channels,\n                    out_channels=conv_channels,\n                    kernel_size=self._kernel_size,\n                    stride=self._stride,\n                    padding=self._padding,\n                )\n            )\n            layers.append(activation)\n            in_channels = conv_channels\n\n        in_length = torch.tensor(input_dim, dtype=torch.float)\n        out_length = calc_length(\n            in_length,\n            padding=self._padding,\n            kernel_size=self._kernel_size,\n            stride=self._stride,\n            ceil_mode=self._ceil_mode,\n            repeat_num=self._sampling_num,\n        )\n        self.out = torch.nn.Linear(conv_channels * int(out_length), feat_out)\n        self.conv = torch.nn.Sequential(*layers)\n\n    def calc_length(\n        self, lengths, padding, kernel_size, stride, ceil_mode, repeat_num=1\n    ):\n        \"\"\"Calculates the output length of a Tensor passed through a convolution or max pooling layer\"\"\"\n        add_pad: float = (padding * 2) - kernel_size\n        one: float = 1.0\n        for i in range(repeat_num):\n            lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one\n            if ceil_mode:\n                lengths = torch.ceil(lengths)\n            else:\n                lengths = torch.floor(lengths)\n        return lengths.to(dtype=torch.int)\n\n    def forward(self, x, lengths):\n        lengths = self.calc_length(\n            lengths,\n            padding=self._padding,\n            kernel_size=self._kernel_size,\n            stride=self._stride,\n            ceil_mode=self._ceil_mode,\n            repeat_num=self._sampling_num,\n        )\n        x = x.unsqueeze(1)\n        x = self.conv(x)\n        b, c, t, f = x.size()\n        x = self.out(x.transpose(1, 2).reshape(b, t, -1))\n        return x, lengths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:43:04.833493Z","iopub.execute_input":"2024-12-26T20:43:04.833802Z","iopub.status.idle":"2024-12-26T20:43:04.842683Z","shell.execute_reply.started":"2024-12-26T20:43:04.833777Z","shell.execute_reply":"2024-12-26T20:43:04.841878Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# **Audio Augmentation**\n  ###  (To Add noise to Audio)","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nimport torchaudio\nfrom torch import nn\nimport math\nimport os\nimport pathlib\nimport numpy as np\n\n\nclass SpeedPerturbation(nn.Module):\n    def __init__(self, sample_rate):\n        super().__init__()\n        self.sample_rate = sample_rate\n\n    @torch.inference_mode()\n    def forward(self, audio_data):\n        #         speed_factor = random.choice([0.9, 1.0, 1.1])\n        speed_factor = np.random.uniform(0.9, 1.1)\n        if speed_factor == 1.0:  # no change\n            return audio_data\n\n        # change speed and resample to original rate:\n        sox_effects = [\n            [\"speed\", str(speed_factor)],\n            [\"rate\", str(self.sample_rate)],\n        ]\n        transformed_audio, _ = torchaudio.sox_effects.apply_effects_tensor(\n            audio_data, self.sample_rate, sox_effects\n        )\n        return transformed_audio\n\n\nclass RandomBackgroundNoise:\n    def __init__(\n        self,\n        noise_dir: str,\n        sample_rate: int,\n        min_snr_db: int = 0,\n        max_snr_db: int = 15,\n    ):\n        self.sample_rate = sample_rate\n        self.min_snr_db = min_snr_db\n        self.max_snr_db = max_snr_db\n\n        if not os.path.exists(noise_dir):\n            raise IOError(f\"Noise directory `{noise_dir}` does not exist\")\n        # find all WAV files including in sub-folders:\n        noise = list(pathlib.Path(noise_dir).glob(\"noise/**/*.wav\"))\n        self.noise_files_list = noise\n        if len(self.noise_files_list) == 0:\n            raise IOError(f\"No .wav file found in the noise directory `{noise_dir}`\")\n\n    def __call__(self, audio_data):\n        random_noise_file = random.choice(self.noise_files_list)\n        effects = [\n            [\"remix\", \"1\"],  # convert to mono\n            [\"rate\", str(self.sample_rate)],  # resample\n        ]\n        noise, _ = torchaudio.sox_effects.apply_effects_file(\n            random_noise_file, effects, normalize=True\n        )\n        audio_length = audio_data.shape[-1]\n        noise_length = noise.shape[-1]\n        if noise_length > audio_length:\n            offset = random.randint(0, noise_length - audio_length)\n            noise = noise[..., offset : offset + audio_length]\n        elif noise_length < audio_length:\n            noise = torch.cat(\n                [noise, torch.zeros((noise.shape[0], audio_length - noise_length))],\n                dim=-1,\n            )\n\n        snr_db = random.randint(self.min_snr_db, self.max_snr_db)\n        snr = math.exp(snr_db / 10)\n        audio_power = audio_data.norm(p=2)\n        noise_power = noise.norm(p=2)\n        scale = snr * noise_power / audio_power\n\n        return (scale * audio_data + noise) / 2\n\n\nclass SpecAugment(nn.Module):\n    def __init__(self, freq_masks=2, time_masks=10, freq_width=27, time_width=0.05):\n        super().__init__()\n        self._rng = random.Random()\n        self.freq_masks = freq_masks\n        self.time_masks = time_masks\n        self.freq_width = freq_width\n        self.time_width = time_width\n        self.mask_value = 0\n\n    @torch.inference_mode()\n    def forward(self, input_spec, length):\n        sh = input_spec.shape\n        for idx in range(sh[0]):\n            for i in range(self.freq_masks):\n                x_left = self._rng.randint(0, sh[2] - self.freq_width)\n                w = self._rng.randint(0, self.freq_width)\n                input_spec[idx, :, x_left : x_left + w] = self.mask_value\n\n            for i in range(self.time_masks):\n                time_width = max(1, int(length[idx] * self.time_width))\n                y_left = self._rng.randint(0, max(1, length[idx] - time_width))\n                w = self._rng.randint(0, time_width)\n                input_spec[idx, y_left : y_left + w, :] = self.mask_value\n        return input_spec, length\n\n\nclass AdaptiveSpecAugment(nn.Module):\n    def __init__(\n        self,\n        freq_masks=2,\n        time_masks=0.05,\n        freq_width=27,\n        time_width=0.05,\n        max_time_masks=10,\n    ):\n        super().__init__()\n        self._rng = random.Random()\n        self.freq_masks = freq_masks\n        self.time_masks = time_masks\n        self.freq_width = freq_width\n        self.time_width = time_width\n        self.max_time_masks = max_time_masks\n        self.mask_value = 0\n\n    @torch.inference_mode()\n    def forward(self, input_spec, length):\n        sh = input_spec.shape\n        for idx in range(sh[0]):\n            for i in range(self.freq_masks):\n                x_left = self._rng.randint(0, sh[2] - self.freq_width)\n                w = self._rng.randint(0, self.freq_width)\n                input_spec[idx, :, x_left : x_left + w] = self.mask_value\n\n            time_masks = min(self.max_time_masks, int(length[idx] * self.time_masks))\n            for i in range(time_masks):\n                time_width = max(1, int(length[idx] * self.time_width))\n                y_left = self._rng.randint(0, max(1, length[idx] - time_width))\n                w = self._rng.randint(0, time_width)\n                input_spec[idx, y_left : y_left + w, :] = self.mask_value\n        return input_spec, length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:43:05.834663Z","iopub.execute_input":"2024-12-26T20:43:05.834954Z","iopub.status.idle":"2024-12-26T20:43:05.849688Z","shell.execute_reply.started":"2024-12-26T20:43:05.834932Z","shell.execute_reply":"2024-12-26T20:43:05.848930Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## **Main Conformer Model**","metadata":{}},{"cell_type":"code","source":"\nclass ConformerModel(nn.Module):\n    def __init__(\n        self,\n        pretrained_conformer,\n        input_dim: int = 80,\n        vocab_size: int = 41,\n        feat_extract_dim: int = 512,\n        conv_channels: int = 256,\n        conformer_dim: int = 1024,\n        dropout_inp_proj: int = 0.1,\n        dropout_outp_proj: int = 0.1,\n        freq_masks=2,\n        time_masks=0.05,\n        freq_width=27,\n        time_width=0.05,\n        max_time_masks=10,\n    ):\n        super().__init__()\n        self.augmentation = AdaptiveSpecAugment(\n            freq_masks, time_masks, freq_width, time_width, max_time_masks\n        )\n        self.conv_subsampling = ConvSubsampling(\n            input_dim=input_dim, feat_out=feat_extract_dim, conv_channels=conv_channels\n        )\n        self.input_projection = nn.Sequential(\n            nn.Linear(feat_extract_dim, conformer_dim), nn.Dropout(dropout_inp_proj)\n        )\n        self.conformer = pretrained_conformer\n        self.output_projection = nn.Sequential(\n            nn.Linear(conformer_dim, conformer_dim),\n            nn.SiLU(),\n            nn.Dropout(dropout_outp_proj),\n            nn.Linear(conformer_dim, vocab_size),\n        )\n\n    def freeze_conformer_blocks(self, n_block: int = 0):\n        for l in range(n_block):\n            for p in self.conformer.layers[l].parameters():\n                p.requires_grad = False\n\n    def forward(\n        self,\n        input_values: Tensor,\n        length: Tensor,\n        attention_mask: Tensor = None,\n        predict: bool = False,\n    ):\n        if not predict:\n            input_values, length = self.augmentation(input_values, length)\n\n        out, length = self.conv_subsampling(input_values, length)\n        hidden_states = self.input_projection(out)\n        out = self.conformer(hidden_states, attention_mask).last_hidden_state\n        out = self.output_projection(out)\n\n        return out, length\n\n    def count_params(self):\n        return sum(p.numel() for p in self.parameters())\n\n    def get_state_dict(self, path_to_ckpt: str):\n        ckpt = torch.load(path_to_ckpt)\n        state = self.load_state_dict(ckpt)\n        print(\"Conformer:\", state)\nprint(\"Model Defined Successfully!!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:43:07.255773Z","iopub.execute_input":"2024-12-26T20:43:07.256121Z","iopub.status.idle":"2024-12-26T20:43:07.265019Z","shell.execute_reply.started":"2024-12-26T20:43:07.256090Z","shell.execute_reply":"2024-12-26T20:43:07.264146Z"}},"outputs":[{"name":"stdout","text":"Model Defined Successfully!!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## **CTC Model**","metadata":{}},{"cell_type":"code","source":"class CTCModel(nn.Module):\n    def __init__(self, conformer_model: nn.Module, text_process):\n        super().__init__()\n        self.model = conformer_model\n        self.ctc_loss = nn.CTCLoss()\n        self.text_process = text_process\n\n    def forward(self, feat, feat_len, target, target_len):\n        out, out_len = self.model(feat, feat_len)\n        if target and target_len:\n            loss = self.criterion(out, target, out_len, target_len)\n            return out, out_len, loss\n        return out, out_len\n\n    def criterion(\n        self,\n        logits: Tensor,\n        targets: Tensor,\n        input_lengths: Tensor,\n        target_lengths: Tensor,\n    ):\n        log_prob = nn.functional.log_softmax(logits, dim=-1)\n        return self.ctc_loss(log_prob, targets, input_lengths, target_lengths)\n\n    def decode(encoder_output: Tensor):\n        argmax = encoder_output.squeeze(0).argmax(-1)\n        return self.text_process.decode(argmax)\n\n    def recognize(inputs: Tensor, input_lengths: Tensor):\n        outputs = list()\n\n        encoder_outputs, _ = self(inputs, input_lengths)\n\n        for encoder_output in encoder_outputs:\n            predict = decode(encoder_output)\n            outputs.append(predict)\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:43:08.316692Z","iopub.execute_input":"2024-12-26T20:43:08.317001Z","iopub.status.idle":"2024-12-26T20:43:08.323406Z","shell.execute_reply.started":"2024-12-26T20:43:08.316977Z","shell.execute_reply":"2024-12-26T20:43:08.322450Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## **Text Process**","metadata":{}},{"cell_type":"code","source":"import json\nclass TextProcess:\n    def __init__(self, dataset=\"libri\", **kwargs):\n        assert dataset in [\"libri\", \"timit\"]\n        \"\"\"label for timit\"\"\"\n        self.base_vocabs = [\n            \"<p>\",\n            \"<s>\",\n            \"<e>\",\n        ]\n        if dataset == \"libri\":\n            vocab = list(\n                json.load(open(\"/kaggle/input/asr-model/phones_mapping.json\", \"r\", encoding=\"utf-8\")).keys()\n            )\n        else:\n            vocab = list(json.load(open(\"/kaggle/input/asr-model/timit_vocab.json\", \"r\")))\n        self.vocabs = self.base_vocabs + vocab\n\n        self.n_class = len(self.vocabs)\n        self.label_vocabs = dict(zip(self.vocabs, range(self.n_class)))\n\n        self.sos_id = 1\n        self.eos_id = 2\n        self.blank_id = 0\n\n    def tokenize(self, data):\n        return data\n\n    def text2int(self, s: str) -> torch.Tensor:\n        return torch.Tensor([self.label_vocabs[i] for i in s])\n\n    def int2text(self, s: torch.Tensor) -> str:\n        text = \"\"\n        for i in s:\n            if i in [self.sos_id, self.blank_id]:\n                continue\n            if i == self.eos_id:\n                break\n            text += \" \" + self.vocabs[i]\n        return text\n\n    def decode(self, argmax: torch.Tensor):\n        \"\"\"\n        decode greedy with collapsed repeat\n        \"\"\"\n        decode = []\n        for i, index in enumerate(argmax):\n            if index != self.blank_id:\n                if i != 0 and index == argmax[i - 1]:\n                    continue\n                decode.append(index.item())\n        return self.int2text(decode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:43:13.607247Z","iopub.execute_input":"2024-12-26T20:43:13.607549Z","iopub.status.idle":"2024-12-26T20:43:13.614908Z","shell.execute_reply.started":"2024-12-26T20:43:13.607525Z","shell.execute_reply":"2024-12-26T20:43:13.613949Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## **Teacher Model Training**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, Tensor, optim\nimport fairseq\nimport transformers\nfrom torchsummary import summary\nimport torchaudio\nfrom transformers import (\n    AutoFeatureExtractor,\n    Wav2Vec2FeatureExtractor,\n    Trainer,\n    Wav2Vec2ConformerForPreTraining,\n)\nimport os\nfrom pathlib import Path\nfrom dataclasses import dataclass, field#Check--OK\nimport random\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple\nimport pandas as pd\nimport jiwer\n# import wandb\nimport time\n# from audio_augmentation import (\n#     SpeedPerturbation,\n#     AdaptiveSpecAugment,\n#     RandomBackgroundNoise,\n# )\n#from dataset import LibriLight # Done\n#from utils import * # Done\n#from text_process import TextProcess# Check This\nimport numpy as np\nfrom transformers import Adafactor\n\nnum_hidden_layers = 2\n\n# wandb.init(\n#     project=\"speech_verification\",\n#     name=f\"conformer_{num_hidden_layers}_hidden_gen_1_libri_subsampling_swish\",\n# )\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n# device = \"cuda:0\"\nprint(\"Device:\", device)\n# device = 'cpu'\n\n\"\"\"# Self-training\"\"\"\n\nbatch_size = 32\n\ntext_process = TextProcess()\n\nn_fft = 1024\nwin_length = 400\nhop_length = 200\nn_mels = 80\n\n# lr = 0.0005 * batch_size ** (1 / 2)\nlr = 0.001\nmax_epochs = 100\nlog_idx = 2\n\nprint(\"upto before Model it is cleared--->!!!!\")\nclass ConformerModel(nn.Module):\n    def __init__(\n        self,\n        pretrained_conformer,\n        input_dim,\n        vocab_size,\n        freq_masks=2,\n        time_masks=0.05,\n        freq_width=27,\n        time_width=0.05,\n    ):\n        super().__init__()\n        # input_dim: 80 (n_mels)\n        feat_extract_dim = 512\n        conv_channels = 256\n        conformer_dim = 1024\n        self.spec_augment = AdaptiveSpecAugment(\n            freq_masks, time_masks, freq_width, time_width\n        )\n        self.conv_subsampling = ConvSubsampling(\n            input_dim=input_dim, feat_out=feat_extract_dim, conv_channels=conv_channels\n        )\n        self.input_projection = nn.Sequential(\n            nn.Linear(feat_extract_dim, conformer_dim), nn.Dropout(0.1)\n        )\n        self.conformer = pretrained_conformer\n        self.output_projection = nn.Sequential(\n            # nn.Linear(conformer_dim, conformer_dim),\n            # nn.Dropout(0.1),\n            nn.SiLU(),\n            nn.Linear(conformer_dim, vocab_size),\n        )\n        self.log_softmax = nn.LogSoftmax(-1)\n    print(\"Model Defined Successfully!!!--->!!!!\")\n    def freeze_conformer_blocks(self):\n        for p in self.conformer.layers[0].parameters():\n            p.requires_grad = False\n    print(\"Model Forwarding Started!!!--->!!!!\")\n    def forward(self, input_values, length, attention_mask=None):\n        out, length = self.spec_augment(input_values, length)\n        out, length = self.conv_subsampling(out, length)\n        hidden_states = self.input_projection(out)\n        out = self.conformer(hidden_states, attention_mask).last_hidden_state\n        out = self.output_projection(out)\n        out = self.log_softmax(out)\n        return out, length\n\n    def count_params(self):\n        return sum(p.numel() for p in self.parameters())\n\n\ndef decode(encoder_output: Tensor) -> str:\n    argmax = encoder_output.squeeze(0).argmax(-1)\n    return text_process.decode(argmax)\n\n\ndef recognize(inputs: Tensor, input_lengths: Tensor, model: nn.Module) -> List[str]:\n    outputs = list()\n\n    encoder_outputs, _ = model(inputs, input_lengths)\n\n    for encoder_output in encoder_outputs:\n        predict = decode(encoder_output)\n        outputs.append(predict)\n\n    return outputs\n\n\ndef train_epoch(model, dataloader, optimizer, scheduler, criterion, epoch):\n    size = len(dataloader)\n    running_loss = 0\n    for batch_idx, batch in enumerate(dataloader):\n        inputs, input_lengths, targets, target_lengths = batch\n        inputs, input_lengths = inputs.to(device), input_lengths.to(device)\n        targets, target_lengths = targets.to(device), target_lengths.to(device)\n\n        outputs, output_lengths = model(inputs, input_lengths)\n\n        loss = criterion(\n            outputs.permute(1, 0, 2), targets, output_lengths, target_lengths\n        )\n\n        if torch.isnan(loss).item() == True:\n            break\n\n        running_loss += loss.item()\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        # wandb.log({\"train/epoch\": epoch})\n        # wandb.log({\"train/loss\": loss.item()})\n        # if scheduler:\n        #     wandb.log({\"train/lr\": scheduler.get_last_lr()[0]})\n        \n    return running_loss / len(dataloader)\n\n\ndef eval_epoch(model, dataloader, criterion, epoch, run_type=\"eval\"):\n    size = len(dataloader)\n    start_time = time.perf_counter()\n    running_loss = 0\n    running_wer = 0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(dataloader):\n            inputs, input_lengths, targets, target_lengths = batch\n            inputs, input_lengths = inputs.to(device), input_lengths.to(device)\n            targets, target_lengths = targets.to(device), target_lengths.to(device)\n\n            outputs, output_lengths = model(inputs, input_lengths)\n\n            loss = criterion(\n                outputs.permute(1, 0, 2), targets, output_lengths, target_lengths\n            )\n\n            predict_sequences = recognize(inputs, input_lengths, model)\n            label_sequences = list(map(text_process.int2text, targets))\n            wer = torch.Tensor(\n                [\n                    jiwer.wer(truth, hypot)\n                    for truth, hypot in zip(label_sequences, predict_sequences)\n                ]\n            )\n            wer = torch.mean(wer).item()\n            running_loss += loss.item()\n            running_wer += wer\n\n            # wandb.log({f\"{run_type}/loss\": loss.item()})\n            # wandb.log({f\"{run_type}/wer\": wer})\n\n    return running_loss / size, running_wer / size\n\n\ntrain_dataset = LibriLight(\n    n_fft=n_fft,\n    n_mels=n_mels,\n    win_length=win_length,\n    hop_length=hop_length,\n    subsets=[\"light\", 'dev-clean', 'dev-other'],\n)\n\ntest_dataset = LibriLight(\n    n_fft=n_fft,\n    n_mels=n_mels,\n    win_length=win_length,\n    hop_length=hop_length,\n    subsets=['test-clean'],\n)\n\n# test_dataset = {}\n# for subset in test_subset:\n#     test_dataset[subset] = LibriLight(\n#         n_fft=n_fft,\n#         n_mels=n_mels,\n#         win_length=win_length,\n#         hop_length=hop_length,\n#         subset=subset,\n#     )\n\n\ndef collate_fn(batch):\n    \"\"\"\n    Take feature and input, transform and then padding it\n    \"\"\"\n\n    specs = [i[0] for i in batch]\n    input_lengths = torch.IntTensor([i.size(0) for i in specs])\n    trans = [i[1] for i in batch]\n    \n    bs = len(specs)\n\n    # batch, time, feature\n    specs = torch.nn.utils.rnn.pad_sequence(specs, batch_first=True)\n\n    trans = [text_process.text2int(s) for s in trans]\n    target_lengths = torch.IntTensor([s.size(0) for s in trans])\n    trans = torch.nn.utils.rnn.pad_sequence(trans, batch_first=True).to(dtype=torch.int)\n\n    return specs, input_lengths, trans, target_lengths\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn=collate_fn,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=2,\n    drop_last=False,\n)\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    collate_fn=collate_fn,\n    shuffle=False,\n    pin_memory=True,\n    num_workers=2,\n)\n\n\n# load pretrained big\nwav2vec2_model = Wav2Vec2ConformerForPreTraining.from_pretrained(\n    \"facebook/wav2vec2-conformer-rel-pos-large\"\n)\nwav2vec2_conformer = wav2vec2_model.wav2vec2_conformer.encoder\nwav2vec2_conformer.layers = nn.ModuleList(\n    [wav2vec2_conformer.layers[i] for i in range(num_hidden_layers)]\n)\n\n\ndef count_params(model):\n    if type(model) == nn.DataParallel:\n        return model.module.count_params()\n    return model.count_params()\n\n\nvocab_size = text_process.n_class\n\nconformer = ConformerModel(wav2vec2_conformer, input_dim=n_mels, vocab_size=vocab_size)\n# if torch.cuda.device_count() > 1:\n#     conformer = nn.DataParallel(conformer, device_ids=[0, 1])\nconformer = conformer.to(device)\n#print(conformer)\n# print(\n#     summary(conformer, [(300, n_mels), (1,)], dtypes=[torch.float, torch.long]) \n# )\nckpt = torch.load('/kaggle/input/nst-pretrained-model/teacher_2_hidden_libri_subsampling_swish_final.pt') # Need to Change This\n\nconformer_state_dict = ckpt['conformer_state_dict']\nconformer.load_state_dict(conformer_state_dict, strict=False)\nprint(\"Load Done!!!\")\n# conformer.load_state_dict(ckpt)\nprint(\"Model Successfully Loaded State Dictionary and Weights!!!!\")\ntotal_steps = len(train_dataloader) * max_epochs\n\nprint(\"Total steps:\", total_steps)\n\ncriterion = nn.CTCLoss().to(device)\noptimizer = optim.AdamW(conformer.parameters(), lr=lr, betas=(0.9, 0.9999))\n\nscheduler = optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=lr, pct_start=0.3, total_steps=total_steps\n)\n# scheduler = None\n\nconfig = {\n    \"learning_rate\": lr,\n    \"max_epochs\": max_epochs,\n    \"batch_size\": batch_size,\n    \"n_fft\": n_fft,\n    \"n_mels\": n_mels,\n    \"num_hidden_layers\": num_hidden_layers,\n    \"dataset\": \"libri-light\",\n    \"no_params\": count_params(conformer),\n    \"augmentation\": f\"SpecAugment\",\n}\n# wandb.config = config\nearly_stopping = EarlyStopping()\n\neval_loss, eval_wer = eval_epoch(\n    conformer, test_dataloader, criterion, 0, 'val'\n)\n\nprint(\"Eval wer:\", eval_wer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:55:43.381783Z","iopub.execute_input":"2024-12-26T20:55:43.382126Z","iopub.status.idle":"2024-12-26T20:58:16.889090Z","shell.execute_reply.started":"2024-12-26T20:55:43.382096Z","shell.execute_reply":"2024-12-26T20:58:16.888109Z"}},"outputs":[{"name":"stdout","text":"Device: cuda:0\nupto before Model it is cleared--->!!!!\nModel Defined Successfully!!!--->!!!!\nModel Forwarding Started!!!--->!!!!\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/wav2vec2-conformer-rel-pos-large were not used when initializing Wav2Vec2ConformerForPreTraining: ['wav2vec2_conformer.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2_conformer.encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing Wav2Vec2ConformerForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ConformerForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ConformerForPreTraining were not initialized from the model checkpoint at facebook/wav2vec2-conformer-rel-pos-large and are newly initialized: ['wav2vec2_conformer.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2_conformer.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-21-af08a0334138>:296: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load('/kaggle/input/nst-pretrained-model/teacher_2_hidden_libri_subsampling_swish_final.pt') # Need to Change This\n","output_type":"stream"},{"name":"stdout","text":"Load Done!!!\nModel Successfully Loaded State Dictionary and Weights!!!!\nTotal steps: 26100\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Eval wer: 6.769102445462855\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}